{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:04:56.987528Z",
     "start_time": "2019-11-22T05:04:56.976544Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:04:57.009516Z",
     "start_time": "2019-11-22T05:04:56.996522Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:09:40.214680Z",
     "start_time": "2019-11-22T05:04:57.016515Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_hdf('../data//tokenized_10thousand.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:09:43.884571Z",
     "start_time": "2019-11-22T05:09:40.331614Z"
    }
   },
   "outputs": [],
   "source": [
    "# json으로 저장한 단어 사전 불러오기\n",
    "import json\n",
    "\n",
    "with open('./id_dict/input_id.json', 'r') as fp:\n",
    "    input_id = json.load(fp)\n",
    "\n",
    "with open('./id_dict/target_id.json', 'r') as fp:\n",
    "    target_id = json.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:11:36.183808Z",
     "start_time": "2019-11-22T05:09:43.896568Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터셋을 아이디값으로 변환(vectorize)\n",
    "original = df.original.apply(lambda y :np.array(list(map(lambda x:input_id[x].setdefault(x,1), y)))).to_numpy()\n",
    "# 한글의 경우 20000개 안에 없는 단어가 있을 수 있다.\n",
    "# OOV 아이디인 1로 바꿔줘야 한다.\n",
    "translation = df.translation.apply(lambda y :np.array(list(map(lambda x:target_id.setdefault(x, 1), y)))).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:11:36.300740Z",
     "start_time": "2019-11-22T05:11:36.224784Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:11:42.339272Z",
     "start_time": "2019-11-22T05:11:36.313735Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_input_data = pad_sequences(original, maxlen=200, padding='post', truncating='post')\n",
    "decoder_target_data = pad_sequences(translation, maxlen=200, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:11:43.478617Z",
     "start_time": "2019-11-22T05:11:42.354269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11, 516, 202, ..., 713, 604, 426],\n",
       "       [ 11,  14, 117, ..., 178,  24,   3],\n",
       "       [ 11, 345, 613, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [ 11, 391,  44, ...,   0,   0,   0],\n",
       "       [ 11, 510,  43, ...,   0,   0,   0],\n",
       "       [ 11, 413, 590, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:12:18.320274Z",
     "start_time": "2019-11-22T05:11:43.503606Z"
    }
   },
   "outputs": [],
   "source": [
    "# decoder input data는 <start> character (== 2)로 시작해야 하며 1 time 씩 밀어내야 한다.\n",
    "tmp = []\n",
    "for t in translation:\n",
    "    a = [2]\n",
    "    a.extend(list(t))\n",
    "    tmp.append(a)\n",
    "tmp = np.array(tmp)\n",
    "\n",
    "decoder_input_data = pad_sequences(tmp, maxlen=200, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:12:18.369209Z",
     "start_time": "2019-11-22T05:12:18.331231Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2,  913,   22, ...,   88,   26,   58],\n",
       "       [   2,   65,    7, ...,   15,    5, 1647],\n",
       "       [   2,  421,   22, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   2,  717,    1, ...,    0,    0,    0],\n",
       "       [   2, 2362,  416, ...,    0,    0,    0],\n",
       "       [   2,  139,    4, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:12:18.423176Z",
     "start_time": "2019-11-22T05:12:18.378203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((99996, 200), (99996, 200), (99996, 200))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape, decoder_target_data.shape, decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:12:18.468152Z",
     "start_time": "2019-11-22T05:12:18.443165Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1  # Batch size for training.\n",
    "epochs = 1  # Number of epochs to train for.\n",
    "latent_dim = 256  # 인코더 차원\n",
    "num_encoder_tokens = 7049 # unique한 한자 캐릭터의 수\n",
    "num_decoder_tokens = 10003 # unique한 한글 토큰의 수\n",
    "embedding_dim = 256 # 워드 임베딩 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:12:18.849931Z",
     "start_time": "2019-11-22T05:12:18.555100Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Softmax\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:12:18.872919Z",
     "start_time": "2019-11-22T05:12:18.859925Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T10:26:17.447739Z",
     "start_time": "2019-11-21T10:24:57.662862Z"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model('./model/s2s_10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T10:26:36.017077Z",
     "start_time": "2019-11-21T10:26:35.544352Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_inputs = model.input[0]   # input_1\n",
    "encoder_embedding = model.layers[2](encoder_inputs)\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[4].output   # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]   # input_2\n",
    "decoder_embedding = model.layers[3](decoder_inputs)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,), name='input_3')\n",
    "decoder_state_input_c = Input(shape=(latent_dim,), name='input_4')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[5]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[6]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:57:59.275874Z",
     "start_time": "2019-11-22T05:57:50.502898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_id = {i:char for char, i in input_id.items()}\n",
    "reverse_input_id[1] = '_'\n",
    "reverse_target_id = {i:char for char, i in target_id.items()}\n",
    "reverse_target_id[1] = '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:57:42.321616Z",
     "start_time": "2019-11-22T05:57:42.215668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OOV'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_input_id[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:17:12.228159Z",
     "start_time": "2019-11-22T05:17:12.008285Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open('./id_dict/reversed_input_id.json','w') as fp:\n",
    "#     json.dump(reverse_input_id, fp)\n",
    "# with open('./id_dict/reversed_target_id.json','w') as fp:\n",
    "#     json.dump(reverse_target_id, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T11:34:57.968798Z",
     "start_time": "2019-11-21T11:34:22.111840Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = 2\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_id[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == 'PAD' or\n",
    "           len(decoded_sentence) > 200):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(1000, 1100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "#     print('-')\n",
    "#     print('Input sentence:', encoder_input_data[seq_index])\n",
    "#     print('Decoded sentence:', decoded_sentence)\n",
    "    with open('./result10.txt', 'a') as fp:\n",
    "        fp.write(decoded_sentence+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T10:43:42.177661Z",
     "start_time": "2019-11-21T10:43:42.138666Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 913,   22, 5822, ...,   26,   58,   39],\n",
       "       [  65,    7, 1972, ...,    5, 1647,   11],\n",
       "       [ 421,   22,  120, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 717,    1,  943, ...,    0,    0,    0],\n",
       "       [2362,  416,    7, ...,    0,    0,    0],\n",
       "       [ 139,    4,   21, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
